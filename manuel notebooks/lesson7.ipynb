{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Galerkin with low-rank tensor representation\n",
    "Recall our parametrized differential equation with solution $u\\in V=L^2(\\Gamma, \\mu; H_0^1(D))$\n",
    "***\n",
    "\\begin{align}\n",
    "\\begin{array}{rclll}\n",
    "-\\operatorname{div}\\kappa(x, y)\\nabla u(x, y) & = & f(x)& \\text{in } D\\times\\Gamma \\\\\n",
    "                                     u(x, y) & = & 0 & \\text{on } \\partial D\\times \\Gamma.\n",
    "\\end{array}                                     \n",
    "\\end{align}\n",
    "***\n",
    "\n",
    "As in the previous section, we need to restrict the approximation space $V$ to something finite dimensional.\n",
    "Here, we employ the truncation $M\\in\\mathbb{N}$ and FE approximation space $X_h$ as before. \n",
    "For the restriction of the generalized polynomial chaos approximation, we consider the full tensor set of order $M$ and degree $\\boldsymbol d=(d_1, \\ldots, d_M)$\n",
    "***\n",
    "\\begin{align}\n",
    "  \\Lambda := \\{\\alpha=(\\alpha_1, \\ldots, \\alpha_M, 0, \\ldots )\\in\\mathbb{N}^\\infty\\;\\vert\\; \\alpha_m=1,\\ldots, d_m, m=1,\\ldots, M\\}.\n",
    "\\end{align}\n",
    "***\n",
    "Then, the discrete space $S_{\\Lambda}^M:= \\mathrm{span}\\{P_\\alpha, \\alpha\\in\\Lambda\\}$, together with $X_h=\\mathrm{span}\\{\\phi_i, i=1,\\ldots, N\\}$ we obtain the approximation space $V_{h, \\Lambda}^M = S_{\\Lambda}^M \\otimes X_h$.\n",
    "Every element $u\\in V_{h, \\Lambda}^M$ can be written in basis representation \n",
    "***\n",
    "\\begin{align}\n",
    "  u(x, y) = \\sum_{i=1}^N\\sum_{\\alpha\\in\\Lambda} U(i, \\alpha)\\phi_i(x)P_\\alpha(y).\n",
    "\\end{align}\n",
    "***\n",
    "The tensor $U\\in\\mathbb{R}^{N, d_1,\\ldots, d_M}$ suffers from the *curse of dimensionality* and its low-rank reduction is our main goal in this section.\n",
    "\n",
    "***\n",
    "The diffusion coefficient in this section is considered to be a random field $\\kappa\\in L^\\infty(\\Gamma^M, \\mu^M; L^\\infty(D))$ that is represented by a polynomial chaos basis in $S_\\Lambda^M$ and FE functions in $X_h$. \n",
    "As before, $\\Gamma^M$ and $\\mu^M$ denote the tensorization of the one-dimensional parameter domain $\\Gamma^M = \\times_{m=1}^M\\Gamma^m$ and measure $\\mu^M=\\otimes_{m=1}^M\\mu^m$, respectively.\n",
    "Moreover, a low-rank assumption is employed, in the sense that there exists a *TT-rank* $\\boldsymbol r=(r_0,\\ldots, r_{M-1})$ such that\n",
    "***\n",
    "\\begin{align}\n",
    "  \\tag{exTT}\n",
    "  \\kappa(x, y) = \\sum_{k_0=1}^{r_0}\\dots\\sum_{k_{M-1}=1}^{r_{M-1}} \\sum_{i=1}^N A_0(i, k_0)\\phi_i(x)\\prod_{m=1}^M \\sum_{\\alpha_m=1}^{d_m}A_m(k_{m-1}, \\alpha_m, k_m)P_{\\alpha_m}(y_m), \\qquad k_M=1 \\text{ is neglected}\n",
    "\\end{align}\n",
    "***\n",
    "This representation is assumed to be exact, otherwise, we need to be more careful in the definition of the bilinear form and the interpretation of the solution. \n",
    "\n",
    "### Example I: affine diffusion \n",
    "The fact that this format is achieved in practical applications can be seen when considering the affine coefficient \n",
    "\\begin{align}\n",
    "  \\tag{1}\n",
    "  \\kappa(x, y) = \\kappa_0(x) + \\sum_{m=1}^M \\kappa_m(x) y_m.\n",
    "\\end{align} \n",
    "This function can be written as the matrix-vector multiplication \n",
    "***\n",
    "\\begin{align}\n",
    "  \\kappa(x, y) = \\underbrace{\\begin{bmatrix} \\kappa_0(x)&\\cdots&\\kappa_{M}(x) \\end{bmatrix}}_{[A_0(x, k_0)]}\n",
    "                  \\underbrace{\\begin{bmatrix}\n",
    "                    1      & 0   & 0 &   \\cdots    & 0 \\\\\n",
    "                    0      & y_1 & 0     & \\cdots & 0 \\\\\n",
    "                    0      & 0   & 1     & \\cdots & 0 \\\\\n",
    "                    \\vdots &  &   & \\ddots      & \\vdots \\\\\n",
    "                    0      &  &  \\cdots    &  & 1 \\\\\n",
    "                  \\end{bmatrix}}_{[A_1(k_0, y_1, k_1]}\n",
    "                  \\underbrace{\\begin{bmatrix}\n",
    "                    1      & 0   & 0 &   \\cdots    & 0 \\\\\n",
    "                    0      & 1 & 0     & \\cdots & 0 \\\\\n",
    "                    0      & 0   & y_2     & \\cdots & 0 \\\\\n",
    "                    \\vdots &  &   & \\ddots      & \\vdots \\\\\n",
    "                    0      &  &  \\cdots    &  & 1 \\\\\n",
    "                  \\end{bmatrix}}_{[A_2(k_1, y_2, k_2)]}\n",
    "                  \\cdots\n",
    "                  \\underbrace{\\begin{bmatrix}\n",
    "                    1      & 0   & 0 &   \\cdots    & 0 \\\\\n",
    "                    0      & 1 & 0     & \\cdots & 0 \\\\\n",
    "                    0      & 0   & 1     & \\cdots & 0 \\\\\n",
    "                    \\vdots &  &   & \\ddots      & \\vdots \\\\\n",
    "                    0      &  &  \\cdots    &  & y_M \\\\\n",
    "                  \\end{bmatrix}\n",
    "                  \\begin{bmatrix} 1\\\\ \\\\ \\vdots \\\\ \\\\ 1 \\end{bmatrix}}_{[A_{M}(k_{M-1}, y_M)]}\n",
    "\\end{align}\n",
    "***\n",
    "which can be interpreted as *rank* $\\boldsymbol r=(M+1, \\ldots, M+1)$ tensor train.\n",
    "This observation is obvious, since the function (1) can be seen as a rank $M+1$ canonical tensor, which has a tensor train representation of rank $\\boldsymbol r$.\n",
    "\n",
    "### Example II: Reconstruction or HOSVD\n",
    "As in the lecture, every high-dimensional tensor can be represented as tensor-train, when applying the higher order singular value decomposition, without truncation. \n",
    "An approximation in a possible low-rank tensor, is obtained when singular value thresholding is considered. \n",
    "As an alternative, the tensor train reconstruction algorithm can be used. \n",
    "This method rely on samples of a function $\\kappa\\colon \\Gamma\\to X_h\\subset X$ and creates a tensor train that minimizes the empirical functional \n",
    "\\begin{align}\n",
    "      \\mathcal{J}(\\Phi) = \\frac{1}{N}\\sum_{i=1}^{N_s}\\|\\Phi(y_i) - \\kappa(y_i)\\|_{X}\n",
    "\\end{align}\n",
    "over the set of all functions having a representation as in (exTT).\n",
    "This procedure is known as *variational Monte-Carlo* method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise: Tensor Reconstruction using torchTT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from dolfin import *\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.sparse as sps\n",
    "import torch\n",
    "\n",
    "_repo_root = os.getcwd()\n",
    "if not os.path.isdir(os.path.join(_repo_root, \"torchTT\")):\n",
    "    _repo_root = os.path.abspath(os.path.join(_repo_root, \"..\"))\n",
    "_torchtt_path = os.path.join(_repo_root, \"torchTT\")\n",
    "if _torchtt_path not in sys.path:\n",
    "    sys.path.insert(0, _torchtt_path)\n",
    "if _repo_root not in sys.path:\n",
    "    sys.path.insert(0, _repo_root)\n",
    "\n",
    "import torchtt\n",
    "from vmc_reconstruction import uq_adf_torchtt as uq\n",
    "from coef_field import CoefField               # CoefField of tutorial 1\n",
    "parameters.linear_algebra_backend = 'Eigen'    # fenics internal matrix system\n",
    "%matplotlib inline\n",
    "\n",
    "M = 5                                          # number of dimensions in coef expansion\n",
    "mean = 1.0                                     # mean value of affine field\n",
    "decay = 2.0                                    # decay parameter of eigenfunctions\n",
    "scale = 1.0                                    # multiplicative scale\n",
    "field = CoefField(M=M, mean=mean, decay=decay, scale=scale)\n",
    "mesh = UnitSquareMesh(10, 10)                  # FEniCS mesh on a square\n",
    "fs = FunctionSpace(mesh, 'CG', 1)              # FunctionSpace definition\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a set of $N_s\\in\\mathbb{N}$ uniform samples in $(-1, 1)^M$ and compute the corresponding field realisations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ns = 100\n",
    "samples = []\n",
    "measurements = []\n",
    "for lia in range(Ns):\n",
    "    y = np.random.rand(M)*2 - 1\n",
    "    value = field.realisation(y, fs).vector().array()\n",
    "    samples.append(y)\n",
    "    measurements.append(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The obtained samples and measurements are given to a torchTT measurement set and the reconstruction is started.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_dim = 5                                 # desired polynomial degree\n",
    "adf_tol = 1e-8                               # solver tolerance\n",
    "adf_iter = 10000                             # maximal number of iteration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "uq_measurements = uq.UQMeasurementSet()      # create measurement set\n",
    "for y, value in zip(samples, measurements):\n",
    "    uq_measurements.add(y, value)\n",
    "basis = uq.PolynomBasis.Legendre             # define flag of used polynomials\n",
    "\n",
    "# set dimensions: [physical, stochastic...]\n",
    "dimensions = [fs.dim()] + [poly_dim] * M\n",
    "res = uq.uq_ra_adf(uq_measurements, basis, dimensions, adf_tol, adf_iter, device=device)\n",
    "print(res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having a look at the terminal output of the notebook, we observe that the error on a Controlset is approximately $10^{-9}$.\n",
    "Some properties of the resulting tensor train are given here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TT-dimension: {}\".format(res.N))\n",
    "print(\"finite element # basis function: {}\".format(fs.dim()))\n",
    "print(\"TT-rank: {}\".format(res.R))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the evaluation of the tensor train in the format as in (exTT) we need a method that takes a sample $y_i$ and returns the corresponding finite element function.\n",
    "Having the polynomial evaluation at hand, we only need to compute the matrix vector product of the tensor components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy.polynomial.legendre as leg        # polynomial evaluation and quadrature\n",
    "def polyval(x, nu, normalised=False):\n",
    "    # the polynomials given by `leg.legval` are not normalized\n",
    "    if normalised:\n",
    "        fn = np.sqrt((2*nu+1))\n",
    "    else:\n",
    "        fn = 1\n",
    "    return leg.legval(x, [0]*nu+[1])*fn\n",
    "\n",
    "def set_fem_fun(vec, fs):\n",
    "    _retval = Function(fs)\n",
    "    _retval.vector().set_local(vec)\n",
    "    return _retval\n",
    "\n",
    "def eval_ex_tt(comps, y, fs, normalised=False):\n",
    "    assert len(y) == len(comps)-1\n",
    "    retval = 1\n",
    "    for lia in range(len(comps)-1, 0, -1):\n",
    "        comp = comps[lia]\n",
    "        values = np.zeros(comp.shape[1])\n",
    "        for lib in range(comp.shape[1]):\n",
    "            values[lib] = polyval(y[lia-1], lib, normalised=normalised)\n",
    "        if lia == len(comps) - 1:\n",
    "            retval = np.dot(comp[:, :, 0], values)\n",
    "            continue\n",
    "        _comp = comp.transpose(0, 2, 1)         # else, transpose [k_m, n, k_m+1] -> [k_m, k_m+1, n]\n",
    "        #                                       # reshape -> [k_m*k_m+1, n]\n",
    "        _comp = _comp.reshape((comp.shape[0] * comp.shape[2], comp.shape[1]), order=\"F\")\n",
    "        _comp = _comp.dot(values)               # dot -> [k_m*k_m+1]\n",
    "        #                                       # reshape -> [k_m, k_m+1]\n",
    "        _comp = _comp.reshape((comp.shape[0], comp.shape[2]), order=\"F\")\n",
    "        retval = _comp.dot(retval)              # dot -> [k_m]\n",
    "    retval = comps[0][0, :, :].dot(retval)\n",
    "    return set_fem_fun(retval, fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We test the result on some samples and show the true coefficient realisations along with the result of the tensor approximation and the absolute difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store component tensors of the torchTT tensor in a list\n",
    "comps = [c.detach().cpu().numpy() for c in res.cores]\n",
    "mpl.rcParams['figure.figsize'] = [10, 3]\n",
    "for lia in range(3):\n",
    "    y = np.random.rand(M)*2 -1 \n",
    "    truth = field.realisation(y, fs)\n",
    "    approx = eval_ex_tt(comps, y, fs)\n",
    "    diff = set_fem_fun(np.abs(truth.vector().array() - approx.vector().array()), fs)\n",
    "    plt.figure()\n",
    "    plt.subplot(131)\n",
    "    plt.title(\"True sample\")\n",
    "    im = plot(truth)\n",
    "    plt.colorbar(im)\n",
    "    plt.subplot(132)\n",
    "    plt.title(\"TT approximation\")\n",
    "    im = plot(approx)\n",
    "    plt.colorbar(im)\n",
    "    plt.subplot(133)\n",
    "    im = plot(diff)\n",
    "    plt.title(\"Difference\")\n",
    "    plt.colorbar(im)\n",
    "    plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting tensor approximation is of high order and very efficiently computed with only $N_s=100$ samples. This is due to the fact, that the affine coefficient is holomorphic and linear in $y$, hence it can be approximated with polynomials with high accuracy.\n",
    "Furthermore, the rank of the tensor approximation is known a priori as maximal $M+1$, which is attained, as one can see above. \n",
    "The decreasing ranks for higher modes $m=2, 3, 4, 5\\;\\; (r_m=5, 4, 3, 2)$ is explained by the employed eigenfunction decay $\\sigma=2$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Galerkin approximation with tensor train coefficient\n",
    "We continue with the formulation of the Galerkin approximation of the model problem, having a diffusion coefficient $\\kappa^{\\mathrm{TT}}$, given as tensor train (exTT).\n",
    "The discrete problem reads:\n",
    "***\n",
    "\n",
    "Find $u_{h} \\in V_{h, \\Lambda}^M$ such that for all $v_h\\in V_{h, \\Lambda}^M$\n",
    "\n",
    "\\begin{align}\n",
    "a(u_h, v_h) = \\int\\limits_{\\Gamma^M}\\int\\limits_{D} \\kappa^{\\mathrm{TT}}(x,y)\\nabla u_h(x,y)\\cdot \\nabla v_h(x,y)\\ \\mathrm{d}x\\ \\mathrm{d}\\mu^M(y)\n",
    "&=\\int\\limits_{\\Gamma^M}\\int\\limits_{D} f(x) v_h(x,y)\\ \\mathrm{d}x\\ \\mathrm{d}\\mu^M(y) = \\ell(v_h).\n",
    "\\end{align}\n",
    "\n",
    "***\n",
    "Employing the tensor format and change the ansatz and test function to basis elements of $V_{h, \\Lambda}^M$ one obtains the discrete bilinear form for every $i, i'=1, \\ldots, N$ and multi-index $\\alpha, \\alpha'i\\in\\Lambda$\n",
    "\n",
    "\\begin{align}\n",
    "  a(i, i', \\alpha, \\alpha') := \\sum_{k_0=1}^{r_0}\\dots \\sum_{k_{M-1}=1}^{r_{M-1}} \\sum_{j=1}^{N}A_0(j, k_0) \\int_D \\phi_j \\nabla\\phi_i\\cdot\\nabla\\phi_{i'}\\mathrm{d}x \\prod_{m=1}^M \\sum_{\\nu_m=1}^{d_m}A_m(k_{m-1}, \\nu_m, k_m) \\int_{\\Gamma^m} P_{\\nu_m}P_{\\alpha_m} P_{\\alpha_m'}\\mathrm{d}\\gamma^m.\n",
    "\\end{align}\n",
    "\n",
    "This formula shows, the choice of basis representation for the diffusion coefficient is not limit to the one that defines $V_{h, \\Lambda}^M$. \n",
    "As in deterministic finite element theory, it is sufficient to choose a discontinuous basis on $D$ with polynomial degree of lower order. \n",
    "The same holds in the stochastic space, where a sufficient basis can be chosen.\n",
    "\n",
    "***\n",
    "For the right-hand side, due to its deterministic character, we obtain\n",
    "\\begin{align}\n",
    "  \\ell(i, \\alpha) = \\int_D f(x)\\phi_{i}(x)\\mathrm{d}x\\prod_{m=1}^M\\int_{\\Gamma^m}P_{\\alpha_m}(y_m)\\mathrm{d}\\gamma^m,\n",
    "\\end{align}\n",
    "where the stochastic integrals are trivial, since the polynomials are chosen orthonormal $P_0(y_m) = 1$.\n",
    "\n",
    "***\n",
    "Finally, the linear system of equations that needs to be solved states: find $U\\in\\mathbb{R}^{N, d_1,\\ldots, d_M}$, such that for every $i=1,\\ldots, N$ and $\\alpha\\in\\Lambda$ \n",
    "\\begin{align}\n",
    "  \\tag{LES}\n",
    "  \\sum_{\\alpha'\\in\\Lambda}\\sum_{i'=1}^N a(i, i', \\alpha, \\alpha') U(i', \\alpha') = \\ell(i, \\alpha).\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise stochastic Galerkin with a tensor train coefficient\n",
    "\n",
    "Implement the stochastic Galerkin system for the diffusion problem with a coefficient, compressed as low-rank tensor train. \n",
    "Solve the algebraic linear system using a TT solver (ALS/AMEn). This torchTT version leaves the solver step as a placeholder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert comps is not None                  # reuse the reconstructed coefficient above\n",
    "coef_comps = comps\n",
    "poly_degs = [4]*M\n",
    "sol_ranks = [1] + [5]*M + [1]\n",
    "coef_ranks = res.R\n",
    "\n",
    "rhs = Constant(1.0)\n",
    "# rhs = Expression(\"sin(2*pi*x[0])*cos(2*pi*x[1]) + x[0]*x[1]\", degree=5)\n",
    "rhs = project(rhs, fs)\n",
    "\n",
    "# Define boundary condition\n",
    "u0 = Constant(0.0)\n",
    "bc = DirichletBC(fs, u0, 'on_boundary')\n",
    "FEM = {\"fs\": fs, \n",
    "       \"bc\": bc}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following method implements the physical component of the discrete bilinear form and linear form. \n",
    "More precisely, for every $k_0=1,\\ldots, r_0$ and $i, i'=1,\\ldots, N$, one stores \n",
    "\\begin{align}\n",
    "  a_0(i, i', k_0) = \\sum_{j=1}^N A_0(j, k_0) \\int_D \\phi_i \\nabla\\phi_i\\cdot \\nabla\\phi_{i'}\\mathrm{d}x\n",
    "\\end{align}\n",
    "and\n",
    "\\begin{align}\n",
    "  \\ell_0(i, 1) = \\int_Df(x)\\phi_i(x)\\mathrm{d}x.\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_fem_comp(coef, rhs, FEM):\n",
    "    assert isinstance(rhs, Function)\n",
    "    fs = FEM[\"fs\"]\n",
    "    bc = FEM[\"bc\"]\n",
    "    u, v = TrialFunction(fs), TestFunction(fs)\n",
    "    \n",
    "    dof2val = bc.get_boundary_values()    \n",
    "    bc_dofs = dof2val.keys()\n",
    "\n",
    "    # setup rhs component\n",
    "    B = []\n",
    "    B_buf = assemble(rhs * v * dx)\n",
    "    bc.apply(B_buf)\n",
    "    B.append(B_buf.array())\n",
    "    B_0 = np.array(B).T\n",
    "    # setup coef components\n",
    "    A_0 = []\n",
    "    for k in range(coef.shape[1]):\n",
    "        coef_fun = set_fem_fun(coef[:, k], fs)\n",
    "        _A = assemble(coef_fun * inner(nabla_grad(u), nabla_grad(v)) * dx)\n",
    "        bc.zero(_A)\n",
    "        rows, cols, values = as_backend_type(_A).data()\n",
    "        A_0.append(sps.csr_matrix((values, cols, rows)))\n",
    "    return A_0, B_0, bc_dofs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to take care of the stochastic components.\n",
    "Hence, the tensor components of the bilinear and linear form are constructed for every dimension $m=1,\\ldots, M$, multi-index $\\alpha, \\alpha'\\in\\Lambda$ and rank $k_{m-1}=1,\\ldots, r_{m-1}$ and $k_m=1,\\ldots, r_m$ as\n",
    "***\n",
    "\\begin{align}\n",
    "  a_m(k_{m-1}, \\alpha_m, \\alpha_m', k_m) = \\sum_{\\nu_m=1}^{d_m}A_m(k_{m-1}, \\alpha_m, \\alpha_m', k_{m})\\int_{\\Gamma^m} P_{\\nu_m}P_{\\alpha_m}P_{\\alpha'_m}\\mathrm{d}\\gamma^m\n",
    "\\end{align}\n",
    "and similar \n",
    "\\begin{align}\n",
    "  \\ell_m(1, \\alpha_m, 1) = \\int_{\\Gamma^m}P_{\\alpha_m}\\mathrm{d}\\gamma^m = \\delta_{1, \\alpha_m}.\n",
    "\\end{align}\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_operator_tt(coef, ranks, degs):\n",
    "    cache_dict = {}                         # use a caching dictionary to not recompute the triple product\n",
    "    def tri_prod_int(mu, nu1, nu2):\n",
    "        if (mu, nu1, nu2) not in cache_dict:\n",
    "            nodes, weights = leg.leggauss(mu + nu1 + nu2 + 1)\n",
    "            # 0.5 for density\n",
    "            # polyval for coefficient (unnormalised Legendre polynomials)\n",
    "            # polyval for ansatz and test functions (normalised Leg. poly)\n",
    "            val = 0.5 * np.sum([w * polyval(n, mu) *\n",
    "                                polyval(n, nu1, normalised=True) * \n",
    "                                polyval(n, nu2, normalised=True) \n",
    "                                for n, w in zip(nodes, weights)], axis=0)\n",
    "            cache_dict[(mu, nu1, nu2)] = val\n",
    "        return cache_dict[(mu, nu1, nu2)]\n",
    "    \n",
    "    bf_cores = [None]\n",
    "    for i in range(len(coef)):\n",
    "        # init coef core (r_{m-1}, d_m, d_m, r_m)\n",
    "        opcorei = np.zeros([ranks[i+1],degs[i],degs[i],ranks[i+2]])\n",
    "        for nu1 in range(degs[i]):\n",
    "            for nu2 in range(degs[i]):\n",
    "                opcorei[:, nu1, nu2, :] = sum([coef[i][:,mu,:]*tri_prod_int(mu, nu1, nu2)\n",
    "                                              for mu in range(coef[i].shape[1])])\n",
    "\n",
    "        bf_cores.append(opcorei)\n",
    "    \n",
    "    lf_cores = [None]\n",
    "    for i in range(len(coef)):\n",
    "        # rhs components are all equal e_1 = [1,0,...,0]\n",
    "        curr_rhs_comp = np.reshape(np.eye(degs[i], 1),\n",
    "                                   [1, degs[i], 1], order='F')\n",
    "        lf_cores.append(curr_rhs_comp)\n",
    "    return bf_cores, lf_cores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To collect the resulting tensor components, we call the written methods and assemble the bilinear and linear form.\n",
    "Note that the bilinear form consists of\n",
    "- a list of $r_0$ sparse matrices: $[a_0(i, i', k_0)] \\in \\mathbb{R}^{N, N, r_0}$\n",
    "- a list of $m=1, \\ldots, M$ order 4 component tensors $[a_m(k_{m-1}, \\alpha_m, \\alpha_m, k_m)]\\in\\mathbb{R}^{r_{m-1}, d_m, d_m, r_m}$.\n",
    "\n",
    "And the linear form is represented similarly using \n",
    "- an assembled linear form $\\ell_0\\in\\mathbb{R}^{1, N, 1}$\n",
    "- a list of $m=1,\\ldots, M$ order 3 component tensors $\\ell_m\\in\\mathbb{R}^{1, d_m, 1}$ with $\\ell_m(1, \\alpha_m, 1) = \\delta_{1, \\alpha_m}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate physical integral parts\n",
    "A_0, B_0, bc_dofs = compute_fem_comp(coef_comps[0][0, :, :], rhs, FEM)\n",
    "# generate stochastic integral parts\n",
    "bf_cores, lf_cores = generate_operator_tt(coef_comps[1:], coef_ranks, poly_degs)\n",
    "\n",
    "# add first physical component to bilinear form\n",
    "bf_cores[0] = []\n",
    "for a_0 in A_0:\n",
    "    bf_cores[0].append(a_0)\n",
    "\n",
    "# add first physical component to linear form\n",
    "lf_cores[0] = np.reshape(B_0, (1, B_0.shape[0], B_0.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The incorporation of the homogenous boundary condition is left.\n",
    "Remember, in the definition of the physical components of the bilinear form, we deleted the rows that correspond to a boundary value, using **bc.zero(_A)**.\n",
    "Hence, we need to set the diagonal entrys of those components to one.\n",
    "This is done by a boundary operator, that is constant in the stochastic components and either zero or one on the diagonal entry corresponding to a (non-) boundary coordinate.\n",
    "Afterwards, this operator is added to the original bilinear form operator.\n",
    "\n",
    "In the torchTT version we keep the operator cores as lists. A custom assembly routine is needed to merge sparse physical cores with dense stochastic cores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create sparse boundary operator\n",
    "bcsparse_direct = sps.csr_matrix(A_0[0])\n",
    "bcsparse_direct.data = np.zeros(len(bcsparse_direct.data))\n",
    "bcsparse_direct[bc_dofs, bc_dofs] = 1\n",
    "# create tensorized boundary operator\n",
    "bcopcores = (M+1) * [[]]\n",
    "bcopcores[0] = [bcsparse_direct]\n",
    "for i in range(M):\n",
    "    bcopcores[i + 1] = np.reshape(np.eye(bf_cores[i + 1].shape[1], bf_cores[i + 1].shape[2]),\n",
    "                                  [1, bf_cores[i + 1].shape[1], bf_cores[i + 1].shape[2], 1], order='F')\n",
    "\n",
    "# Build the TT operator cores for the bilinear form on CPU.\n",
    "# First core: stack physical stiffness matrices for each coefficient rank.\n",
    "A0_dense = np.stack([a.toarray() if sps.issparse(a) else np.asarray(a) for a in A_0], axis=-1)\n",
    "A0_core = A0_dense.reshape(1, A0_dense.shape[0], A0_dense.shape[1], A0_dense.shape[2])\n",
    "A_cores = [torch.tensor(A0_core, dtype=torch.float64)]\n",
    "for i in range(1, len(bf_cores)):\n",
    "    A_cores.append(torch.tensor(bf_cores[i], dtype=torch.float64))\n",
    "A_tt = torchtt.TT(A_cores)\n",
    "\n",
    "# Boundary operator cores (rank-1 operator).\n",
    "bc0_dense = bcsparse_direct.toarray()\n",
    "bc_cores = [torch.tensor(bc0_dense.reshape(1, bc0_dense.shape[0], bc0_dense.shape[1], 1), dtype=torch.float64)]\n",
    "for i in range(1, len(bcopcores)):\n",
    "    bc_cores.append(torch.tensor(bcopcores[i], dtype=torch.float64))\n",
    "BC_tt = torchtt.TT(bc_cores)\n",
    "\n",
    "tt_bf = A_tt + BC_tt\n",
    "\n",
    "# Build TT right-hand side.\n",
    "b_cores = [torch.tensor(lf_cores[0], dtype=torch.float64)]\n",
    "for i in range(1, len(lf_cores)):\n",
    "    b_cores.append(torch.tensor(lf_cores[i], dtype=torch.float64))\n",
    "tt_lf = torchtt.TT(b_cores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the system of equations needs to be solved.\n",
    "Since we do not want to solve the system (LES) directly for it is of high-dimensionality, we employ a minimisation approach.\n",
    "This means, instead of solving (LES), we look for a tensor train $U\\in\\mathbb{R}^{N, d_1,\\ldots, d_M}$ with\n",
    "***\n",
    "\\begin{align}\n",
    "  U = \\mathrm{argmin} \\|\\mathcal{A}U-\\mathcal{F} \\|^2_F,\n",
    "\\end{align}\n",
    "***\n",
    "where $\\mathcal{A}$ and $\\mathcal{F}$ are the computed tensor train bilinear and linear form.\n",
    "\n",
    "The method of choice is an alternating least squares (ALS) method.\n",
    "\n",
    "torchTT does not provide the original ALS solver or sparse TT operator conversion, so we leave the solver step as a placeholder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve with AMEn (ALS-type) solver from torchTT.\n",
    "# Use the Python implementation to avoid C++ dependency.\n",
    "sol_tt = torchtt.solvers.amen_solve(\n",
    "    tt_bf,\n",
    "    tt_lf,\n",
    "    nswp=20,\n",
    "    eps=1e-8,\n",
    "    rmax=max(sol_ranks),\n",
    "    use_cpp=False,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "print(\"Solution computed!\")\n",
    "print(\"TT-dimension: {}\".format(sol_tt.N))\n",
    "print(\"finite element # basis function: {}\".format(fs.dim()))\n",
    "print(\"TT-rank: {}\".format(sol_tt.R))\n",
    "\n",
    "sol_comps = [c.detach().cpu().numpy() for c in sol_tt.cores]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final result is an approximation to the original problem in low-rank tensor train format on the discrete space $V_{h, \\Lambda}^M$, namely\n",
    "\\begin{align}\n",
    "  u(x, y) \\approx u^{TT}(x, y) = \\sum_{\\alpha\\in\\Lambda}\\sum_{i=1}^N U(i, \\alpha)\\phi_i(x)P_{\\alpha}(y).\n",
    "\\end{align}\n",
    "And written as tensor train\n",
    "\\begin{align}\n",
    "  u^{TT}(x,y) = \\sum_{k_0=1}^{r_0}\\dots \\sum_{k_{M-1}=1}^{r_{M-1}}\\sum_{i=1}^N U_0(i, k_0) \\phi_i(x)\\prod_{m=1}^M \\sum_{\\alpha_m=1}^{d_m}U_m(k_{m-1}, \\alpha_m, k_m)P_{\\alpha_m}(y_m).\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us compare the result. \n",
    "As for the coefficient field, we consider samples of the solution. \n",
    "On the one hand, as if we would have solved the partial differential equation with the specific field realisation, and on the other hand, as an evaluation of the tensor train result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pw_solver(y):\n",
    "    u, v = TrialFunction(fs), TestFunction(fs)    \n",
    "    _coef = field.realisation(y, fs)\n",
    "    _L = _coef * inner(nabla_grad(u), nabla_grad(v))*dx(fs.mesh())\n",
    "    _f = rhs * v * dx(fs.mesh())\n",
    "    _u = Function(fs)\n",
    "    solve(_L == _f, _u, bc)\n",
    "    return _u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if sol_comps is None:\n",
    "    print(\"Skipping solution plots: no TT solution computed.\")\n",
    "else:\n",
    "    mpl.rcParams['figure.figsize'] = [12, 3]\n",
    "    for lia in range(3):\n",
    "        y = np.random.rand(M)*2 -1 \n",
    "        true_coef = field.realisation(y, fs)\n",
    "        truth = pw_solver(y)\n",
    "        approx = eval_ex_tt(sol_comps, y, fs, normalised=True)\n",
    "        diff = set_fem_fun(np.abs(truth.vector().array() - approx.vector().array())/np.linalg.norm(truth.vector().array()), fs)\n",
    "        plt.figure()\n",
    "        plt.subplot(141)\n",
    "        plt.title(\"True Coef\")\n",
    "        im = plot(true_coef)\n",
    "        plt.colorbar(im)\n",
    "        plt.subplot(142)\n",
    "        plt.title(\"True sample\")\n",
    "        im = plot(truth)\n",
    "        plt.colorbar(im)\n",
    "        plt.subplot(143)\n",
    "        plt.title(\"TT approximation\")\n",
    "        im = plot(approx)\n",
    "        plt.colorbar(im)\n",
    "        plt.subplot(144)\n",
    "        im = plot(diff)\n",
    "        plt.title(\"Difference\")\n",
    "        plt.colorbar(im)\n",
    "        plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To show off, let us demonstrate the importance of a surrogate by making a timing test. \n",
    "Therefore, we aim at the creation of 1000 samples for the solution, once for the true solution, and once for the surrogate evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "if sol_comps is None:\n",
    "    print(\"Skipping timing tests: no TT solution computed.\")\n",
    "else:\n",
    "    N = 1000\n",
    "    y_list = []\n",
    "    for lia in range(N):\n",
    "        y_list.append(np.random.rand(M)*2 - 1)\n",
    "\n",
    "    mean_true = np.zeros(fs.dim())\n",
    "    start = time.time()\n",
    "    for y in y_list:\n",
    "        mean_true += pw_solver(y).vector().array()\n",
    "    mean_true *= N**(-1)\n",
    "    end = time.time()\n",
    "    print(\"Time needed (true): {}\".format(end - start))\n",
    "\n",
    "    mean_approx = np.zeros(fs.dim())\n",
    "    start = time.time()\n",
    "    for y in y_list:\n",
    "        mean_approx += eval_ex_tt(sol_comps, y, fs, normalised=True).vector().array()\n",
    "    mean_approx *= N**(-1)\n",
    "    end = time.time()\n",
    "    print(\"Time needed (approx): {}\".format(end - start))\n",
    "\n",
    "    print(\"difference of mean: {}\".format(np.linalg.norm(mean_true - mean_approx)/np.linalg.norm(mean_true)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One main feature of the extended tensor train format, i.e. the representation in an explicit basis (orthonormal polynomials) is the fact, that integration is fairly simple.\n",
    "For example, if we want to compute the mean, we have to integrate over the tensor train\n",
    "***\n",
    "\\begin{align}\n",
    "  \\mathbb{E}\\left[u^{TT}(x, \\cdot)\\right] = \\int_{\\Gamma^M}u^{TT}(x, y)\\mathrm{d}\\gamma(y) = \\sum_{k_0=1}^{r_0}\\dots\\sum_{k_{M-1}=1}^{r_{M-1}} \\sum_{i=1}^NU_0(i, k_0)\\phi_i(x) \\prod_{m=1}^M\\sum_{\\alpha_m=1}^{d_m}A_m(k_{m-1}, \\alpha_m, k_m)\\int_{\\Gamma^m}P_{\\alpha_m}(y_m)\\mathrm{d}\\gamma^m(y_m).\n",
    "\\end{align}\n",
    "***\n",
    "Again, using the orthonormality of the basis, one can reduce the stochastic integrals to a Kronecker-delta\n",
    "***\n",
    "\\begin{align}\n",
    "  \\tag{TT trick}\n",
    "  \\mathbb{E}\\left[u^{TT}(x, \\cdot)\\right] = U_0(x)\\odot U_1(0) \\odot \\cdots \\odot U_M(0).\n",
    "\\end{align}\n",
    "***\n",
    "Here, we mean with $\\odot$ the usual matrix-dot product and $U_m(0)$ is the component matrix $[U_m(k_{m-1}, 0, k_m)]_{k_{m-1}=1, \\ldots, r_{m-1}}^{k_m=1,\\ldots, r_m}\\in\\mathbb{R}^{r_{m-1}, r_m}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if sol_comps is None:\n",
    "    print(\"Skipping TT-trick mean: no TT solution computed.\")\n",
    "else:\n",
    "    start = time.time()\n",
    "    tt_mean = [1]\n",
    "    for lia in range(len(sol_comps)-1, -1, -1):\n",
    "        if lia > 0:\n",
    "            comp = sol_comps[lia][:, 0,: ]\n",
    "        else:\n",
    "            comp = sol_comps[0][0, :, :]\n",
    "        tt_mean = np.dot(comp, tt_mean)\n",
    "    end = time.time()\n",
    "    print(\"Time needed (TT trick): {}\".format(end - start))\n",
    "    print(\"difference of mean (true vs TT trick): {}\".format(np.linalg.norm(mean_true - tt_mean)/np.linalg.norm(mean_true)))\n",
    "    print(\"difference of mean (approx vs TT trick): {}\".format(np.linalg.norm(mean_approx - tt_mean)/np.linalg.norm(mean_approx)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "End of part VI. Next: *Bayesian inversion*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right; color: #a5a9af\"> &copy; Manuel Marschall 2018-2019</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}