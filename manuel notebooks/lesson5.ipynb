{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to torchTT\n",
    "This section gives an overview of tensor basics and torchTT usage.\n",
    "For that let\n",
    "\\begin{align}\n",
    "A \\in \\mathbb{R}^{n_1 \\times \\dots \\times n_d}\n",
    "\\end{align}\n",
    "be a tensor of order $d$ and $i$th-mode dimension $n_d$.\n",
    "\n",
    "For $d = 4$ and $n_i = 2$ it could look like the following.\n",
    "\n",
    "## Creating a Tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "from numpy import size, zeros\n",
    "\n",
    "_torchtt_path = os.path.join(os.getcwd(), \"torchTT\")\n",
    "if _torchtt_path not in sys.path:\n",
    "    sys.path.insert(0, _torchtt_path)\n",
    "import torchtt\n",
    "\n",
    "def frob_norm(t):\n",
    "    return torch.linalg.norm(t).item()\n",
    "\n",
    "def tt_datasize(tt):\n",
    "    return sum(int(c.numel()) for c in tt.cores)\n",
    "\n",
    "A = torch.rand((2,2,2,2), dtype=torch.float64)\n",
    "print(A)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you see, torch prints an order $4$ tensor as nested arrays. Internally tensors are stored in row-major format. This is:\n",
    "\\begin{align}\n",
    "A(i_1,\\dots,i_d) = \\sum_{k=1}^d (\\prod_{l=k+1}^{d} n_l )i_k\n",
    "\\end{align}\n",
    "With this you can excess elements directly:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(A[0,0,0,0])              # first entry in all dimensions\n",
    "print(A.flatten()[0])          # first entry in 1D array format\n",
    "print(A[0,0,1,1])              # Entry (0, 0, 1, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this sense tensors are just arrays. torchTT adds low-rank tensor formats later.\n",
    "The two standard creations of tensors are the following:\n",
    "\n",
    "**all zeros**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.zeros((2,2,2), dtype=torch.float64)\n",
    "print(A)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**identity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.eye(2, dtype=torch.float64)\n",
    "print(A)\n",
    "A = torch.eye(4, dtype=torch.float64).reshape(2,2,2,2)\n",
    "print(A)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doing stuff with Tensors\n",
    "\n",
    "In all above cases, the frobenius Norm\n",
    "\\begin{align}\n",
    "\\| A \\|_F = \\sqrt{\\sum_i A(i_1,\\dots,i_d)^2}\n",
    "\\end{align}\n",
    "\n",
    "can be calculated like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.rand((2,2,2,2), dtype=torch.float64)\n",
    "print(frob_norm(A))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensors are linear objects, so you can add them (if the dimensions agree) and do scalar multiplication:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.rand((2,2,2), dtype=torch.float64)\n",
    "print(A)\n",
    "B = torch.rand((2,2,2), dtype=torch.float64)\n",
    "print(B)\n",
    "print(A + 2*B)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But most importantly, you can multiply them. For tensor contractions we use Einstein summation via torch.einsum (or torch.tensordot).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensor index notation is expressed with torch.einsum / torch.tensordot in torch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, if you want to do for example matrix vector multiplication you can do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.zeros((2,2), dtype=torch.float64)\n",
    "A[0,1] = 1\n",
    "A[1,0] = 1\n",
    "print('A = \n ' + str(A))\n",
    "x = torch.tensor([-1.0, 1.0], dtype=torch.float64)\n",
    "print('x = \n ' + str(x))\n",
    "b = A @ x\n",
    "print('b = \n ' + str(b))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But it does not stop here. You can do all weird kinds of multiplications and you only need to write Einstein Notation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.ones((2,2,2,2), dtype=torch.float64)\n",
    "print('A = \n ' + str(A))\n",
    "x = torch.zeros((2,2,2), dtype=torch.float64)\n",
    "x[0,0,0] = -1\n",
    "x[1,0,0] = 1\n",
    "x[1,1,0] = 2\n",
    "print('x = \n ' + str(x))\n",
    "b = torch.tensordot(A, x, dims=([1,2,3],[0,1,2]))\n",
    "print('b = \n ' + str(b))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, you can do a basis transformation \n",
    "\\begin{align}\n",
    "\\Sigma = P^T A P\n",
    "\\end{align}\n",
    "like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.zeros((2,2), dtype=torch.float64)\n",
    "A[1,0] = 1\n",
    "A[0,1] = 1\n",
    "P = torch.zeros((2,2), dtype=torch.float64)\n",
    "P[0,0] = -1/np.sqrt(2)\n",
    "P[1,0] = 1/np.sqrt(2)\n",
    "P[0,1] = 1/np.sqrt(2)\n",
    "P[1,1] = 1/np.sqrt(2)\n",
    "\n",
    "S = P @ A @ P.T\n",
    "\n",
    "print('A = \n' + str(A))\n",
    "print('P = \n' + str(P))\n",
    "print('S = \n' + str(S))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see transposing is a not that important concept in tensor mthematic. You just transform each index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doing more complex stuff\n",
    "You can even do a singular value decomposition of a Tensor just by using the index notation. How you distribute the indices defines what kind of singular value decomposition you do want. I.e.: For $A\\in \\mathbb{R}^{4 \\times 4 \\times 3 \\times 2} $ an SVD could look like that (depending on the chosen matrification, here $n_1n_2 \\times n_3n_4$):\n",
    "\\begin{align}\n",
    "A(i_1,i_2,i_3,i_4) =  \\sum_{k=1}^r U(i_1,i_2,k) S(k,k) V(k,i_3,i_4) = \\sum_{k_1,k_2=1}^r U(i_1,i_2,k_1) S(k_1,k_2) V(k_2,i_3,i_4), \n",
    "\\end{align}\n",
    "where $r$ is the rank of this matrification and $U$ and $V^t$ have orthogonal columns. This operation is the key to everything what happens on tensors (HOSVD, HSVD etc.). In torch use torch.linalg.svd.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.ones((4,4,3,2), dtype=torch.float64)\n",
    "A_mat = A.reshape(4*4, 3*2)\n",
    "U, S, Vh = torch.linalg.svd(A_mat, full_matrices=False)\n",
    "S_mat = torch.diag(S)\n",
    "U_t = U.reshape(4,4,-1)\n",
    "V_t = Vh.reshape(-1,3,2)\n",
    "\n",
    "print('S = \n '  + str(S_mat))\n",
    "print('U = \n '  + str(U_t))\n",
    "print('V = \n '  + str(V_t))\n",
    "A_test = torch.einsum(\"abk,kl,lcd->abcd\", U_t, S_mat, V_t)\n",
    "print('Test if A can be recovered: ' + str(frob_norm(A - A_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also choose the following matrification:\n",
    "\\begin{align}\n",
    "A(i_1,i_2,i_3,i_4) =  \\sum_{k=1}^r U(i_1,k) S(k,k) V(k,i_2,i_3,i_4) = \\sum_{k_1,k_2=1}^r U(i_1,k_1) S(k_1,k_2) V(k_2,i_2,i_3,i_4), \n",
    "\\end{align}\n",
    "In torch, this translates to:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.ones((4,4,3,2), dtype=torch.float64)\n",
    "A_mat = A.reshape(4, 4*3*2)\n",
    "U, S, Vh = torch.linalg.svd(A_mat, full_matrices=False)\n",
    "S_mat = torch.diag(S)\n",
    "U_t = U.reshape(4,-1)\n",
    "V_t = Vh.reshape(-1,4,3,2)\n",
    "\n",
    "print('S = \n '  + str(S_mat))\n",
    "print('U = \n '  + str(U_t))\n",
    "print('V = \n '  + str(V_t))\n",
    "A_test = torch.einsum(\"ak,kl,libc->aibc\", U_t, S_mat, V_t)\n",
    "print('Test if A can be recovered: ' + str(frob_norm(A - A_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor Trains (choo choo)\n",
    "\n",
    "The Tensor Train format is most facinating. It helps you to find a tractable low rank representation (if possible) for a given Tensor. In formulars it looks like:\n",
    "\\begin{align}\n",
    "A(i_1,\\dots,i_d) = \\sum_{k_1,\\dots,k_{d-1} = 1}^{r_1,\\dots,r_{d-1}} A_1(i_1,k_1) A_2(k_1,i_2,k_2)\\dots A_{d-1}(k_{d-2},i_{d-1},k_{d-1})  A_d(k_{d-1},i_d) = A_1(i_1)\\dots  A_d(i_d)\n",
    "\\end{align}\n",
    "The last representation is the reason why in physics and elsewhere it is called Matrix Product State (or MPS).\n",
    "In torchTT you can create a random Tensor Train with 5 'legs' and dimension 3 and ranks 2 like this. The components are order 3 tensors with ranks (2,3,2) except the first and last component. Below you see the components and their mode dimensions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_TT = torchtt.random([3,3,3,3,3], 2)\n",
    "\n",
    "for i in range(0,5):\n",
    "    print(A_TT.cores[i])\n",
    "    print(A_TT.cores[i].shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HT Tensors (here them grow (because they are trees))\n",
    "HT tensors are not implemented in torchTT. The linearity of tensor trains might not be the best way of capturing the low-rank approximability of a given tensor. What you see below is the result of initializing a random HT tensor and applying the hierarchical SVD in the original tutorial.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HTTensor is not supported in torchTT.\n",
    "A_HT = None\n",
    "print(\"HTTensor example skipped (torchTT does not implement HT tensors).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the orthogonality "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HTTensor component operations are not supported in torchTT.\n",
    "print(\"HTTensor component example skipped.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, the norm of the core (root) tensor is the same as the norm of the whole Tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HTTensor norms are not supported in torchTT.\n",
    "print(\"HTTensor norm example skipped.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A simple ALS implementation on the Tensor Train format\n",
    "To do linear algebra on tensors formats like tensor train you can use alternating schemes, like ALS (alternating least squared). You iteratively project your big problem on each component solve there and then substitute the component with the 'local' solution. For some reason that works quite well! (It can be shown that the residuum is monotonic decreasing, but that does not imply convergence all the time). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we import numpy and specify the number of components of our Tensor train (dim), the size of each 'leg' (n) the maxmal number of ranks of our operator (r) and our solution (r+1), the error magin (eps) and the maximal number of iterations (max_iter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "dim = 8\n",
    "n = 4\n",
    "r = 2\n",
    "eps = 10e-8\n",
    "max_iter = 100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we define some indices to create a random symmetric Tensor train operator (A). The multiplication is to make the operator symmetric. Then we cast a random solution and calculate a right hand side (b). In the end we have \n",
    "\\begin{align}\n",
    "A x = b, A \\in \\mathbb{R}^{(n \\times \\dots \\times n) \\times (n \\times \\dots \\times n)}, b  \\in \\mathbb{R}^{n \\times \\dots \\times n}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 8\n",
    "n = 4\n",
    "r = 2\n",
    "A = torchtt.random([(n, n)] * dim, 2)\n",
    "\n",
    "solution = torchtt.random([n for _ in range(dim)], r+1)\n",
    "b = A @ solution\n",
    "\n",
    "print(torch.sqrt(torchtt.dot(b, b)).item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we initialize a random starting vector $x$. And move the core to the first component with move_core."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torchtt.random([n for _ in range(dim)], r+1)\n",
    "# torchTT does not expose move_core; use rounding/orthogonalization as needed.\n",
    "x = x.round(1e-12)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the ALS iteration is performed. There are max_iter sweeps from left to right. At each sweep we touch each TT component once. We build the projection operators on the component and build the 'local' operator (op) and the local right hand side (rhs). Then, we transform the tensor to a numpy array, reshape it, solve the system of equations and create a new TT core from the solution (sol). This is the new component. Finally we check if the error in the Frobenius norm is smaller than eps. If so, we stop.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The ALS micro-iteration shown in the original tutorial is not implemented here.\n",
    "# Consider torchtt.solvers.amen_solve for TT linear systems instead.\n",
    "print(\"ALS micro-iteration example skipped (not implemented in torchTT).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right; color: #a5a9af\"> &copy; Michael G\u00f6tte, Robert Gruhlke, Manuel Marschall, Phillip Trunschke, 2018-2019</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}